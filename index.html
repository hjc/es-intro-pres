<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>A Light Introduction to Elasticsearch</title>

		<meta name="description" content="A quick presentation teaching you about ES.">
		<meta name="author" content="Hayden Chudy">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Elasticsearch</h1>
					<h3>A Short Introduction</h3>
					<p>
						<small>Created by <a href="http://haydenchudy.com">Hayden Chudy</a> / <a href="http://twitter.com/hjc1710">@hjc1710</a></small>
						<small>
							<button onclick="RevealNotes.open()">
								Open Notes
							</button>
						</small>
					</p>
				</section>

				<section>
					<h2>What is Elasticsearch?</h2>
					<ul>
						<li>
							A distributed, clusterable search server powered by a JSON DSL for searching.
						</li>
						<li>
							Really, just a layer of abstractions on top of Lucene to store more structured data.
						</li>
						<li>
							Can also be used as a simple document-oriented NoSQL database.
							<ul>
								<li>Sort of like Mongo with better queries and even <b>less</b> relations.</li>
								<li>Normally backed by another database storage engine though.</li>
							</ul>
						</li>
						<li>Written in Java.</li>
						<li>
							Created by Shay Banon in 2010.
						</li>
					</ul>

					<aside class="notes">
						<ul>
							<li>
								Layer of abstractions - Lucene only stores strings, but ES supports storing, indexing
								and searching through complicated JSON structures, this is all done through clever use
								of key names and values within Lucene, that ES turns into a structure of sort
							</li>
							<li>
								Some people use ES as their primary storage engine, especially if their core data source
								is logs. However, ES is frequently joined with another database storage engine, and data
								is mirrored between the two.
							</li>
						</ul>
					</aside>
				</section>

				<!-- Example of nested vertical slides -->
				<section>
					<h2>What is Lucene?</h2>
					<ul>
						<li>Open Source project by Apache to write an Information Retrieval (IR) System.</li>
						<li>IR Systems use metadata and full text search to make finding information very efficient.</li>
						<li>Provides a robust and accurate scoring algorithm, in addition to fast queries.</li>
						<li>A number of major search engine solutions are built on top of Lucene.</li>
						<li>Started in 1999 by Doug Cutting, adopted by Apache in 2001, became it's own top level project in 2005.</li>
						<li>Also written in Java, providing a robust native Java API.</li>
					</ul>

					<aside class="notes">
						<ul>
							<li>
								IR Systems are focused on efficiently searching for and retrieving information, as
								opposed to being focused on efficiently storing said information (which is what normal
								DBMS's are focused on).
							</li>
							<li>
								For example: a common strategy in IR to improve search robustness is to index the
								same field of data in multiple, different fashions so it can match more complex
								queries. Compare this to relational DB, where you are constantly trying to lower how
								many duplicate entries of a field there are.
							</li>
							<li>
								The scoring algorithm used is: tf-idf. However, individual query objects can affect
								tf-idf scoring and lie at the heart of why Lucene scoring is great.
							</li>
							<li>
								Large projects built on top of Lucene, in addition to ES, include:
								<ul>
									<li>Solr</li>
									<li>Compass (precursor to ES)</li>
									<li>
										Swiftype (an enterprise search start up that sells search solutions to other sites,
										built on top of Lucene)
									</li>
									<li>KinoSearch (another big search server like Solr)</li>
								</ul>
							</li>
							<li>
								Some people even use Lucene directly and circumvent the likes of Solr and ES entirely.
								Examples of this include: Apple.com, LinkedIn, Jira, and, formerly, Twitter.
							</li>
							<li>ES utilizes Lucene's Java API to provide fast, native access.</li>
						</ul>
					</aside>
				</section>

				<section>
					<h2>Let's Begin!</h2>
				</section>

				<section>
					<h2>Installing Elasticsearch</h2>
					<ul>
						<li>Use the Vagrantfile in this repository.</li>
						<li>Use your Operating System's package manager:</li>
					</ul>
					<pre><code data-trim>
# latest on Ubuntu
$ apt-get install openjdk-7-jdk
$ wget -qO-  http://packages.elasticsearch.org/GPG-KEY-elasticsearch - | apt-key add -
$ echo "deb http://packages.elasticsearch.org/elasticsearch/1.7/debian stable main" > /etc/apt/sources.list.d/elasticsearch.list
$ apt-get update
$ apt-get install elasticsearch
# OSX with Brew
$ brew install elasticsearch
					</code></pre>
					<aside class="notes">
						<ul>
							<li>Vagrant requires version 1.6+, vagrant-bindfs, and vagrant-salt.</li>
							<li>Install those and vagrant up, and vagrant will forward port 9200 for you.</li>
						</ul>
					</aside>
				</section>

				<section>
					<h2>Seeing if it Works</h2>
					<pre><code data-trim>
hayden@beardtop ~> curl -XGET localhost:9200
{
	"status" : 200,
	"name" : "Madam Slay",
	"cluster_name" : "elasticsearch",
	"version" : {
		"number" : "1.7.1",
		"build_hash" : "b88f43fc40b0bcd7f173a1f9ee2e97816de80b19",
		"build_timestamp" : "2015-07-29T09:54:16Z",
		"build_snapshot" : false,
		"lucene_version" : "4.10.4"
	},
	"tagline" : "You Know, for Search"
}
					</code></pre>
					<aside class="notes">
						<ul>
							<li>
								Seeing if ES is up and running is simple: just curl port 9200 and see if you get
								a response!
							</li>
							<li>Fun fact, all Elasticsearch node names come from Marvel super hero names.</li>
						</ul>
					</aside>
				</section>

				<section>
					<h2>Indexing Your First Item</h2>
					<pre><code data-trim>
hayden@beardtop ~> curl -XPOST "localhost:9200/croscon/employees/1" -d '{
	"name": "Tom Sawyer",
	"id": 1,
	"specialties": ["javascript", "php"]
}'
{"_index":"croscon","_type":"employees","_id":"1","_version":1,"created":true}

hayden@beardtop ~> curl -XGET "localhost:9200/croscon/employees/1"
{"_index":"croscon","_type":"employees","_id":"1","_version":1,"found":true,"_source":{
	"name": "Tom Sawyer",
	"id": 1,
	"specialties": ["javascript", "php"]
}}
					</code></pre>

					<aside class="notes">
						<ul>
							<li>Indexing is as simple as a POST request.</li>
							<li>
								We explicitly specified the id in the path, but you can also omit it and let ES
								create one on its own.
							</li>
							<li>Fetching is then as simple as GETing the route by id.</li>
							<li>
								The URL scheme goes as follows: $ES_URL/$INDEX_NAME/$TYPE_NAME/$DOCUMENT_ID.
								<ul>
									<li>
										An index is a collection of documents that have somewhat similar characteristics.
										For example, you can have an index for customer data, and another index for a
										product catalog.
									</li>
									<li>
										A type is a logical category/partition of your index whose structure is up to you.
										In general, a type is defined for documents that have a set of common fields.
										For example, letâ€™s assume you run a blogging platform and store all your data
										in a single index. In this index, you may define a type for user data,
										another type for blog data, and yet another type for comments data.
									</li>
									<li>
										A document is a basic unit of information that can be indexed.
										For example, you can have a document for a single customer, another document
										for a single product, and yet another for a single order.
										This document is expressed in JSON, much like Mongo.
										<br/>
										Within an index/type, you can store as many documents as you want. Note
										that although a document physically resides in an index, a document actually
										must be indexed/assigned to a type inside an index.
									</li>
								</ul>
							</li>
						</ul>
					</aside>
				</section>

				<section>
					<h2>Updating Data</h2>
					<pre><code data-trim>
hayden@beardtop ~> curl -XPUT "localhost:9200/croscon/employees/1" -d '{
	"name": "Tom Sawyer",
	"id": 1,
	"specialties": ["javascript", "php"],
	"date_of_birth": "1990-12-10"
}'
{"_index":"croscon","_type":"employees","_id":"1","_version":2,"created":false}

hayden@beardtop ~> curl -XGET "localhost:9200/croscon/employees/1"
{"_index":"croscon","_type":"employees","_id":"1","_version":2,"found":true,"_source":{
	"name": "Tom Sawyer",
	"id": 1,
	"specialties": ["javascript", "php"],
	"date_of_birth": "1990-12-10"
}}
					</code></pre>
					<aside class="notes">
						Let's add a new date of birth field.
						<br/>
						To update an item, you merely PUT to the route by ID with the new, complete
						document. Deleting of fields is done by omission.
						<br/>
						Brand new fields are automatically mapped into the index, parsed, and saved.
						<br/>
						Known fields are parsed as described by their mappings, and saved.
						<br/>
						<br/>
						Some more employee data:
						{ "name": "Ben Rogers", "id": 2, "specialties": ["css", "less", "magic"], "date_of_birth": "1987-08-10" }
						{ "name": "Huck Finn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17" }
						{ "name": "Pap Finn", "id": 4, "specialties": ["php", "python", "devops", "magic"], "date_of_birth": "1984-07-16" }
					</aside>
				</section>

				<section>
					<section>
						<h2>Searching</h2>

						<h3>Getting Everyone</h3>
						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon/employees/_search?pretty'
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "4",
      "_score" : 1.0,
      "_source":{ "name": "Pap Finn", "id": 4, "specialties": ["php", "python", "devops", "magic"], "date_of_birth": "1984-07-16" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "Tom Sawyer","id": 1,"specialties": ["javascript", "php"],"date_of_birth": "1990-12-10"}
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{ "name": "Ben Rogers", "id": 2, "specialties": ["css", "less", "magic"], "date_of_birth": "1987-08-10" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{ "name": "Huck Finn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17" }
    } ]
  }
}
						</code></pre>

						<aside class="notes">
							The presence of that `_search` is integral. Running a search against just an index
							and a type will produce an error along the lines of: `No endpoint found for
							$TYPENAME`.
							<br/>
							If you'll notice, these aren't in any real order, and every document returned
							has a score of: 1.0.
							<br/>
							When searching, `?pretty` is your friend, otherwise, all your results are returned
							minified.
						</aside>
					</section>
					<section>
						<h2>Searching</h2>

						<h3>Getting just the PHP'rs</h3>

						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon/employees/_search?pretty' -d '
{
  "query": {
    "term": {
      "specialties": "php"
    }
  }
}'

{
  "hits" : {
    "total" : 3,
    "max_score" : 0.19178301,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "1",
      "_score" : 0.19178301,
      "_source":{ "name": "Tom Sawyer","id": 1,"specialties": ["javascript", "php"],"date_of_birth": "1990-12-10" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "4",
      "_score" : 0.15342641,
      "_source":{ "name": "Pap Finn", "id": 4, "specialties": ["php", "python", "devops", "magic"], "date_of_birth": "1984-07-16" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "3",
      "_score" : 0.15342641,
      "_source":{ "name": "Huck Finn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17" }
    } ]
  }
}

						</code></pre>
						<aside class="notes">
							And everyone but Ben is returned, as expected.
							<br/>
							If you'll notice, we searched for a string in an array rather easily, and ES will naturally
							make all of its queries just <i>work</i> with arrays.
							<br/>
							If you notice, the score has changed! Tom Sawyer wins because he has the least
							specialties!
						</aside>
					</section>
					<section>
						<h2>Searching</h2>
						<h3>Getting everyone born in 1990 and on:</h3>
						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon/employees/_search' -d '
{
  "query": {
    "range": {
      "date_of_birth": {
        "gte": "1990-01-01",
        "lte": "now"
      }
    }
  }
}'
{
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{ "name": "Tom Sawyer", "id": 1, "specialties": ["javascript", "php"],"date_of_birth": "1990-12-10" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{ "name": "Huck Finn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17" }
    } ]
  }
}

						</code></pre>
						<aside class="notes">
							The `lte` for now is optional, but it is included to show you that ES has some strings
							with special meaning. This one turns into the current DateTime.
							<br/>
							You'll notice there's still no score here. That's because scoring a range query is
							<b>hard</b> and elasticsearch doesn't do <b>everything</b>
							<br/>
							These are all just very light examples, and all of the options given can be further
							customized to support boosting, multiple date formats, timeszones, etc.
							<br/>
							Segue into mapping with a note about: "But, wait... how did that date range search work?
							We just gave it a string? There were no explicit dates involved!"
						</aside>
					</section>
				</section>

				<section>
					<h2>Welcome to Mappings</h2>
					<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/_mapping?pretty'
{
  "croscon" : {
    "mappings" : {
      "employees" : {
        "properties" : {
          "date_of_birth" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "id" : {
            "type" : "long"
          },
          "name" : {
            "type" : "string"
          },
          "specialties" : {
            "type" : "string"
          }
        }
      }
    }
  }
}
					</code></pre>
					<aside class="notes">
						<ul>
							<li>
								Behind the scenes, ES creates a mapping for each type it has in an index.
								It parses each JSON payload and makes an intelligent best guess at what
								type each field should be.
								<br/>
								If you'll notice, it guessed correctly that our date_of_birth field was a
								Date with an optional Time part, even though it was sent as a string.
								<br/>
								Mapping also works on nested objects and arrays, natively.
							</li>
							<li>
								Mapping is what make the magic ES queries possible, and also efficient.
							</li>
							<li>
								Some supported types: string, number, date, boolean, binary (base64 representation
								of binary data; not stored or indexed by default), arrays, objects, arrays of objects,
								ip addresses (IPv4 only), geographic points, and geographic shapes.
							</li>
							<li>
								The type of a field determines which queries can be run on it and how given queries
								are processed/run. For example, doing a range over numbers is done very differently
								then a range over IP's.
							</li>
						</ul>
					</aside>
				</section>

				<section>
					<section>
						<h2>Explicit Mappings</h2>
						<ul>
							<li>
								Either a file in <code>/etc/elasticsearch/templates</code> or dynamically PUT via an
								API request.
							</li>
							<li>
								Example:
							</li>
						</ul>
						<aside class="notes">
							While ES does determine automatic mappings for each type, you can also explicitly define
							a mapping for each type yourself, in case of ambiguous fields, complicated fields (such as
							multi-fields), or missing fields.
						</aside>
					</section>
					<section>
						<pre><code data-trim>
hayden@beardtop ~> curl -XPUT 'localhost:9200/croscon/_mapping/projects' -d '
{
  "projects": {
    "_id": {
      "index": "not_analyzed",
      "path": "id",
      "type": "long"
    },
    "properties": {
      "name": {
        "type": "string",
        "store": true,
        "index": "analyzed",
        "fields": {
          "raw": {
            "type": "string",
            "index": "not_analyzed"
          }
        }
      },
      "due_date": {
        "type": "date"
      },
      "id": {
        "type": "long"
      },
      "client": {
        "type": "object",
        "properties": {
          "id": {
            "type": "long"
          },
          "name": {
            "type": "string"
          }
        }
      }
    }
  }
}'
						</code></pre>
						<aside class="notes">
							<ul>
								<li>
									Multi-fields: Multi-fields are when you pass a given field through multiple
									analyzers and store multiple variations of it, for different searching reasons.
									A very common multi-field, is the `raw` field, which is just a string that is
									indexed as is, and is not parsed before being indexed.
								</li>
								<li>
									Stored vs. indexed.
									<br/>
									For any given field, you can choose to store and/or index it.
									<br/>
									Storing a field means it is stored directly alongside the document identifier.
									Think of it this way: Lucene just stores pointers to ID's, these ID's are then
									looked up in ES, and come with extra information. By default, the indexed JSON document
									is stored in the _source field and it can be parsed and used to return any field
									you desire. Alternatively, you can disable _source, to save space, or store
									an additional field explicitly, allowing you to fetch it without parsing _source.
									<br/>
									Indexing a field means it is inserted into Lucene and prepared for searching. To
									search over a field it <b>must</b> be indexed. You can index a field as analyzed
									or not_analyzed. Analyzed means it will be transformed before being inserted (more
									on that later), not analyzed means it is inserted into Lucene raw.
									<br/>
									By default, fields are indexed and not stored.
								</li>
							</ul>
						</aside>
					</section>
					<section>
						<h2>Breaking Your Mappings</h2>

						If you ever try to insert data that doesn't match your mappings, ES will slap you back
						with this:

						<pre><code data-trim>
hayden@beardtop ~> curl -XPOST 'localhost:9200/croscon/employees' -d '
{
  "name": "Ben Rogers",
  "id": 2,
  "specialties": ["css", "less", "magic"],
  "date_of_birth": "1987-XX-YY"
}'
{"error":"MapperParsingException[failed to parse [date_of_birth]];
nested: MapperParsingException[failed to parse date field
[1987-XX-YY], tried both date format [dateOptionalTime], and
timestamp number with locale []]; nested:
IllegalArgumentException[Invalid format: \"1987-XX-YY\" is
malformed at \"-XX-YY\"]; ","status":400}
						</code></pre>

						<aside class="notes">
							I will point out though, that this is a very useful error message.
						</aside>
					</section>
				</section>

				<section>
					<h2>TO THE MOON WITH SCORING AND ORDERING!</h2>

					<img src="img/doge-moon.png" alt="TO THE MOON!!!!!!!!!"/>
				</section>

				<section>
					<section>
						<h2>Ordering</h2>
						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon/projects/_search' -d '
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "due_date": {
        "order": "asc",
        "missing": "_last"
      }
    },
    {
      "name.raw": {
        "order": "desc"
      }
    },
    "_score"
  ]
}'
{
  "hits" : {
    "total" : 3,
    "max_score" : null,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "MC3 Rearch", "due_date": "2015-09-08", "id": 1, "client": { "name": "HFA", "id": 1 } },
      "sort" : [ 1441670400000, "MC3 Rearch", 1.0 ]
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"name": "MC4", "due_date": "2016-12-21", "id": 2, "client": { "name": "Croscon", "id": 2 } },
      "sort" : [ 1482278400000, "MC4", 1.0 ]
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{"name": "MC5", "due_date": null, "id": 3, "client": null },
      "sort" : [ 9223372036854775807, "MC5", 1.0 ]
    } ]
  }
}

						</code></pre>

						<aside class="notes">
							<ul>
								<li>
									Earlier, we lightly discussed that Lucene implemented multiple scoring algorithms.
									Internally, Elasticsearch uses these derived scores to determine the sort order for
									all results.
								</li>
								<li>
									However, we can override that sort order rather easy. We can sort on our arbitrary
									fields in whatever order we want, then we can let Elasticsearch's native score
									ordering kick in whenever.
								</li>
								<li>
									The order syntax is rather simple, but extendable:
									<ul>
										<li>
											Pick a field name, and then a direction.
										</li>
										<li>
											Alternatively, pick a field name, then open an object for more options! In
											the second example we're doing two things:
											<ol>
												<li>
													We're using our name.raw multi-field. We're using this because names
													are composed of two parts, a first and a last, and we want to sort on
													both of those names. If we were to run this over just the `name` field,
													we would get unpredictable results, due to how analyzers
													work (something we'll cover later).
												</li>
												<li>
													We're using the special `missing` option. `missing` tells ES what to do
													when it finds a document without this field. Rather, it tells ES what
													VALUE to give that document for this field. In this case, we're telling
													ES to give that document whatever value it needs to be sorted last
													(likely a name of: ""). If we wanted all missing names to be sorted as
													if they were equal to: "CROSCON", that would be as simple as changing
													"_last" to be "CROSCON" (an example of that would be nice).
												</li>
											</ol>
										</li>
										<li>
											Our final example is the score sorting fallback. If you just pass `_score`,
											then ES knows to sort on the score in a descending fashion! AWESOME!
										</li>
									</ul>
								</li>
							</ul>
						</aside>
					</section>
					<section>
						<h2>Ordering</h2>
						<pre><code data-trim>
 curl -XGET 'localhost:9200/croscon/projects/_search?pretty' -d '
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "due_date": {
        "order": "asc",
        "missing": "_first"
      }
    },
    {
      "name.raw": {
        "order": "desc"
      }
    },
    "_score"
  ]
}'
{
  "hits" : {
    "total" : 3,
    "max_score" : null,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{"name": "MC5", "due_date": null, "id": 3, "client": null },
      "sort" : [ -9223372036854775808, "MC5", 1.0 ]
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "MC3 Rearch", "due_date": "2015-09-08", "id": 1, "client": { "name": "HFA", "id": 1 } },
      "sort" : [ 1441670400000, "MC3 Rearch", 1.0 ]
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"name": "MC4", "due_date": "2016-12-21", "id": 2, "client": { "name": "Croscon", "id": 2 } },
      "sort" : [ 1482278400000, "MC4", 1.0 ]
    } ]
  }
}

						</code></pre>
						<aside class="notes">
							We can also make missing fields come first.s
						</aside>
					</section>

					<section>
						<h2>Ordering</h2>
						<pre><code data-trim>
curl -XGET 'localhost:9200/croscon/projects/_search?pretty' -d '
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "due_date": {
        "order": "asc",
        "missing": 1444455518000
      }
    },
    {
      "name.raw": {
        "order": "desc"
      }
    },
    "_score"
  ]
}'
{
  "hits" : {
    "total" : 3,
    "max_score" : null,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "MC3 Rearch", "due_date": "2015-09-08", "id": 1, "client": { "name": "HFA", "id": 1 } },
      "sort" : [ 1441670400000, "MC3 Rearch", 1.0 ]
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{"name": "MC5", "due_date": null, "id": 3, "client": null },
      "sort" : [ 1444455518000, "MC5", 1.0 ]
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"name": "MC4", "due_date": "2016-12-21", "id": 2, "client": { "name": "Croscon", "id": 2 } },
      "sort" : [ 1482278400000, "MC4", 1.0 ]
    } ]
  }
}
						</code></pre>
						<aside class="notes">
							Or we can give it a specific value. In this example, we've given it the UNIX timestamp
							that corresponds to: 2015-10-10, putting it right in the middle
						</aside>
					</section>
				</section>

				<section>
					<h2>Scoring</h2>

					<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/tweets/tweet/_search?pretty' -d '
{
  "query": {
	"match": {
	  "tweet": "taylorswift13"
	}
  }
}'
{
  "hits" : {
    "total" : 2,
    "max_score" : 0.095891505,
    "hits" : [ {
      "_index" : "tweets",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : **0.095891505**,
      "_source":{"user": "@hjc1710", "tweet": "Not convinced @carlyraejepsen is better than @taylorswift13 though."}
    }, {
      "_index" : "tweets",
      "_type" : "tweet",
      "_id" : "4",
      "_score" : **0.076713204**,
      "_source":{"user": "@hjc1710", "tweet": "You'll always have my heart @taylorswift13, no matter what @carlyraejepsen does."}
    } ]
  }
}
					</code></pre>

					<aside class="notes">
						<ul>
							<li>
								The other part of ordering results is scoring. Scoring can basically be summed up as
								"Lucene gives a document a relevance score, indicating how relevant said document
								was to our initial search".
							</li>
							<li>
								Obviously, this is a wonderful thing to sort on.
							</li>
							<li>
								The starred portion are the scores elasticsearch has derived. Since we haven't boosted
								much, they're rather small, but you'll notice they're different. The reason the first
								tweet is higher is because it is a shorter string and matches in shorter strings are
								worth more, under the argument that the less words there are, the more impact each
								individual word has on the sentence.
							</li>
							<li>
								But, how the fuck does it work!?
							</li>
						</ul>
					</aside>
				</section>

				<section>
					<section>
						<h2>A Smidge of TF-IDF</h2>
						<ul>
							<li>
								TF-IDF is the algorithm that is the primary driving force behind Lucene's relevance score.
							</li>
							<li>
								TF-IDF = "Term Frequency - Inverse Document Frequency"
							</li>
							<li>
								Basically: the more a word appears in a single document, the more valuable it is; however
								the more it appears in MULTIPLE documents all in a single index, the LESS valuable it is.
							</li>
							<li>
								This sort of naturally handles things like Stop Words (and, the, is), but there are even
								better solutions to that problem later!
							</li>
						</ul>
					</section>

					<section>
						<img src="img/tfidf.png" alt="Image of TF IDF Formula" style="height: 600px;"/>

						<aside class="notes">
							Mathematically, the way this all works all plays out kind of like:
							<ol>
								<li>
									Calculate the term frequency. This can be done multiple ways, but the easiest
									way is to make it so term frequency == the raw frequency of a term in a document.
									Basically, if f(t, d) is the raw number of times a single term appears in a document,
									then tf(t, d) = f(t, d). Other, more complicated schemes apply, such as logarithmically
									scaled term frequencies [tf(t, d) = 1 + log(f(t, d))], or even augmented frequencies
									which prevent bias towards longer documents
									[tf(f, d) = 0.5 + ((0.5 x f(t, d)) / max(all_docs, key=f(t, d)))
								</li>
								<li>
									Calculate the inverse document frequency. The inverse document frequency is gotten
									by taking the log of the number of total documents in a corpus (or index, or collection
									of indices) divided by the total number of documents that contain this term, so:
									log(N / abs(sum(N, key=lambda x: term in x.terms)))
								</li>
								<li>
									Multiply those two.
								</li>
							</ol>
							In a nutshell: Maths. Mucho, mucho maths.
							<br/>
							Lucene does this for every query, and it does it an efficient manner!
						</aside>
					</section>

					<aside class="notes">

					</aside>
				</section>

				<section>
					<section>
						<h2>Peeking Into Scoring</h2>
						Let's see how elasticsearch scores:

						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/tweets/tweet/_search?pretty&explain' -d '
{
  "query": {
	"match": {
	  "tweet": "carlyraejepsen"
	}
  }
}'
{
  "hits" : {
    "total" : 4,
    "max_score" : 0.11506981,
    "hits" : [ {
      "_shard" : 2,
      "_node" : "08j3yVwyRaCJ3PQrQBcy3A",
      "_index" : "tweets",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.11506981,
      "_source":{"user": "@hjc1710", "tweet": "The new @carlyraejepsen album is top notch"},
      "_explanation" : {
        "value" : 0.11506981,
        "description" : "weight(tweet:carlyraejepsen in 0) [PerFieldSimilarity], result of:",
        "details" : [ {
          "value" : 0.11506981,
          "description" : "fieldWeight in 0, product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "tf(freq=1.0), with freq of:",
            "details" : [ {
              "value" : 1.0,
              "description" : "termFreq=1.0"
            } ]
          }, {
            "value" : 0.30685282,
            "description" : "idf(docFreq=1, maxDocs=1)"
          }, {
            "value" : 0.375,
            "description" : "fieldNorm(doc=0)"
          } ]
        } ]
      }
    },
    {
      "_source": {"user": "@hjc1710", "tweet": "Not convinced @carlyraejepsen is better than @taylorswift13 though."},
      "_explanation" : {
        "value" : 0.095891505,
        "description" : "weight(tweet:carlyraejepsen in 0) [PerFieldSimilarity], result of:",
        "details" : [ {
          "value" : 0.095891505,
          "description" : "fieldWeight in 0, product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "tf(freq=1.0), with freq of:",
            "details" : [ {
              "value" : 1.0,
              "description" : "termFreq=1.0"
            } ]
          }, {
            "value" : 0.30685282,
            "description" : "idf(docFreq=1, maxDocs=1)"
          }, {
            "value" : 0.3125,
            "description" : "fieldNorm(doc=0)"
          } ]
        } ]

...
						</code></pre>
						<aside class="notes">
							<ul>
								<li>
									If you throw the `explain` GET parameter into the query string, ES will return
									a HUGE amount of info on how the scoring went. In addition to other things.
								</li>
								<li>
									Included in here is TF-IDF information! Also, information on the field length norm,
									which is how long the field is, and is why the longer string was worth less earlier!
								</li>
								<li>
									Notice how the IDF score is the same for both documents (because we're searching
									for the same term), and it's really the field norm that differentiates them here.
								</li>
								<li>
									Other info ES returns about the query include:

									<ol>
										<li>
											The Lucene Shard that the data came from (more on that later).
										</li>
										<li>
											The ID of the node that found this result.
										</li>
									</ol>

									Why is this returned? Because term and document frequencies are counted per
									shard! Not per index!
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>Let's boost!</h2>

						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/tweets/tweet/_search?pretty&explain' -d '
{
  "query": {
	"bool": {
	  "should": [
		{
		  "match": {
			"tweet": {
			  "query": "taylorswift13",
			  "boost": 2
			}
		  }
		},
		{
		  "match": {
			"tweet": "carlyraejepsen"
		  }
		}
	  ]
	}
  }
}'
{
  "hits" : {
    "total" : 4,
    "max_score" : 0.12865195,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "08j3yVwyRaCJ3PQrQBcy3A",
      "_index" : "tweets",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 0.12865195,
      "_source":{"user": "@hjc1710", "tweet": "Not convinced @carlyraejepsen is better than @taylorswift13 though."},
      "_explanation" : {
        "value" : 0.12865196,
        "description" : "sum of:",
        "details" : [ {
          "value" : 0.08576798,
          "description" : "weight(tweet:taylorswift13^2.0 in 0) [PerFieldSimilarity], result of:",
          "details" : [ {
            "value" : 0.08576798,
            "description" : "score(doc=0,freq=1.0), product of:",
            "details" : [ {
              "value" : 0.89442724,
              "description" : "queryWeight, product of:",
              "details" : [ {
                "value" : 2.0,
                "description" : "boost"
              }, {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 1.4574206,
                "description" : "queryNorm"
              } ]
            }, {
              "value" : 0.095891505,
              "description" : "fieldWeight in 0, product of:",
              "details" : [ {
                "value" : 1.0,
                "description" : "tf(freq=1.0), with freq of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "termFreq=1.0"
                } ]
              }, {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 0.3125,
                "description" : "fieldNorm(doc=0)"
              } ]
            } ]
          } ]
        }, {
          "value" : 0.04288399,
          "description" : "weight(tweet:carlyraejepsen in 0) [PerFieldSimilarity], result of:",
          "details" : [ {
            "value" : 0.04288399,
            "description" : "score(doc=0,freq=1.0), product of:",
            "details" : [ {
              "value" : 0.44721362,
              "description" : "queryWeight, product of:",
              "details" : [ {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 1.4574206,
                "description" : "queryNorm"
              } ]
            }, {
              "value" : 0.095891505,
              "description" : "fieldWeight in 0, product of:",
              "details" : [ {
                "value" : 1.0,
                "description" : "tf(freq=1.0), with freq of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "termFreq=1.0"
                } ]
              }, {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 0.3125,
                "description" : "fieldNorm(doc=0)"
              } ]
            } ]
          } ]
        } ]
      }
    }, {
      "_shard" : 0,
      "_node" : "08j3yVwyRaCJ3PQrQBcy3A",
      "_index" : "tweets",
      "_type" : "tweet",
      "_id" : "4",
      "_score" : 0.10292157,
      "_source":{"user": "@hjc1710", "tweet": "You'll always have my heart @taylorswift13, no matter what @carlyraejepsen does."},
      "_explanation" : {
        "value" : 0.10292157,
        "description" : "sum of:",
        "details" : [ {
          "value" : 0.06861438,
          "description" : "weight(tweet:taylorswift13^2.0 in 0) [PerFieldSimilarity], result of:",
          "details" : [ {
            "value" : 0.06861438,
            "description" : "score(doc=0,freq=1.0), product of:",
            "details" : [ {
              "value" : 0.89442724,
              "description" : "queryWeight, product of:",
              "details" : [ {
                "value" : 2.0,
                "description" : "boost"
              }, {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 1.4574206,
                "description" : "queryNorm"
              } ]
            }, {
              "value" : 0.076713204,
              "description" : "fieldWeight in 0, product of:",
              "details" : [ {
                "value" : 1.0,
                "description" : "tf(freq=1.0), with freq of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "termFreq=1.0"
                } ]
              }, {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 0.25,
                "description" : "fieldNorm(doc=0)"
              } ]
            } ]
          } ]
        }, {
          "value" : 0.03430719,
          "description" : "weight(tweet:carlyraejepsen in 0) [PerFieldSimilarity], result of:",
          "details" : [ {
            "value" : 0.03430719,
            "description" : "score(doc=0,freq=1.0), product of:",
            "details" : [ {
              "value" : 0.44721362,
              "description" : "queryWeight, product of:",
              "details" : [ {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 1.4574206,
                "description" : "queryNorm"
              } ]
            }, {
              "value" : 0.076713204,
              "description" : "fieldWeight in 0, product of:",
              "details" : [ {
                "value" : 1.0,
                "description" : "tf(freq=1.0), with freq of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "termFreq=1.0"
                } ]
              }, {
                "value" : 0.30685282,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 0.25,
                "description" : "fieldNorm(doc=0)"
              } ]
            } ]
          } ]
        } ]
      }
    },
    {
      "_shard" : 4,
      "_node" : "08j3yVwyRaCJ3PQrQBcy3A",
      "_index" : "tweets",
      "_type" : "tweet",
      "_id" : "3",
      "_score" : 0.005816851,
      "_source":{"user": "@hjc1710", "tweet": "Yea, I think I like 1989 more than anything @carlyraejepsen has done."},
      "_explanation" : {
        "value" : 0.005816851,
        "description" : "product of:",
        "details" : [ {
          "value" : 0.011633702,
          "description" : "sum of:",
          "details" : [ {
            "value" : 0.011633702,
            "description" : "weight(tweet:carlyraejepsen in 0) [PerFieldSimilarity], result of:",
            "details" : [ {
              "value" : 0.011633702,
              "description" : "score(doc=0,freq=1.0), product of:",
              "details" : [ {
                "value" : 0.15165187,
                "description" : "queryWeight, product of:",
                "details" : [ {
                  "value" : 0.30685282,
                  "description" : "idf(docFreq=1, maxDocs=1)"
                }, {
                  "value" : 0.49421698,
                  "description" : "queryNorm"
                } ]
              }, {
                "value" : 0.076713204,
                "description" : "fieldWeight in 0, product of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "tf(freq=1.0), with freq of:",
                  "details" : [ {
                    "value" : 1.0,
                    "description" : "termFreq=1.0"
                  } ]
                }, {
                  "value" : 0.30685282,
                  "description" : "idf(docFreq=1, maxDocs=1)"
                }, {
                  "value" : 0.25,
                  "description" : "fieldNorm(doc=0)"
                } ]
              } ]
            } ]
          } ]
        }, {
          "value" : 0.5,
          "description" : "coord(1/2)"
        } ]
      }
    } ]
  }
}


						</code></pre>
						<aside class="notes">
							Legit, just walk through this.
							<br/>
							This is a truncated resultset too.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Analyzers, Tokenizers, and TokenFilters</h2>

						<ul>
							<li>
								Analyzers, Tokenizers, and TokenFilters allow you to transform your textual data into a more
								searchable format.
							</li>
							<li>
								An Analyzer merely consists of a series of Tokenizers and TokenFilters.
							</li>

							<li>
								Customizing Analyzers, TokenFilters, and Tokenizers is quite possible, but is FAR beyond the scope of this talk.
							</li>
							<li>
								Example analyzers include: standard, whitespace, and language.
							</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>
									Analyzers are what make elasticsearch powerful and good for searching. They
									take your very specific sentences and turn them into more generic and searchable
									pieces.
								</li>
								<li>
									Example Analyzer:
									<ul>
										<li>
											Standard Analyzer: breaks words into tokens based on UAX #29, lowercases
											everything, and removes stop words.
										</li>
										<li>
											Whitespace Analyzer: Splits words into tokens based on whitespace.
										</li>
										<li>
											Language: An implementation of the standard analyzer for other languages
											besides English. Supported languages include: arabic, persian, portuguese,
											and norwegian.
										</li>
									</ul>
								</li>
								<li>
									When we're creating that `name.raw` field, all we're telling ES to do is... DON'T
									analyze that field at all, so we have the full string in tact and can work with it.
								</li>
								<li>
									Analyzers are also why sorting on non-raw fields gives odd results. When this is done
									ES sorts on the first token it finds, which is basically undefined behavior.
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>Tokenizers</h2>

						<ul>
							<li>
								Tokenizers take a string of text and split it up into individual tokens, each
								of which is indexed and can be searched for.
							</li>
							<li>
								Some example Tokenizers include: standard, keyword, ngram, and pattern.
							</li>
							<li>
								Example ngram:
								<pre><code data-trim>
ngram("croscon", max=5, min=4) == [
  "cros",
  "crosc",
  "rosc",
  "rosco",
  "osco",
  "oscon",
  "scon"
]
								</code></pre>
							</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>
									Tokenizers split a single string into multiple, smaller constituent pieces,
									each of which is indexed and can be searched for.
								</li>
								<li>
									Tokenizers are the crux of search. Without tokenizers, you would have to search for
									an exact string to get any match out of ES. With tokenizers, ES will break each word
									in a string up into its own item, allowing you to search for just that.
								</li>
								<li>
									Example tokenizers:
									<ul>
										<li>
											Standard - a tokenizer good for most European languages, implements
											the Unicode Text Segmentation algorithm which is described in Unicode
											Annex #29, which describes guidelines for determining default boundaries
											between major text elements (characters, words, and sentences). I did not
											have the time to fully read through the algorithm, but it was badass.
										</li>
										<li>
											Keyword - indexes the entire string as is.
										</li>
										<li>
											NGram - breaks words up into smaller pieces, as specified by you. For
											example, you set a ngram tokenizer up with a max ngram of 5 and min of
											4 and pass it 'croscon', you get: cros, crosc, rosc, rosco, osco, oscon,
											and scon. This is effective in breaking words up for partial matches. You
											pick the characters eligible for ngramming.
										</li>
										<li>
											Pattern - breaks up strings into tokens that match a regex. Example:
											regex of: \$\d+\.\d+, with string of: "Cost: $25.00" would create
											a single token of: $25.00.
										</li>
									</ul>
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>TokenFilters</h2>
						<ul>
							<li>
								TokenFilters further parse the results of tokens, stripping unwanted tokens and
								transforming tokens into more searchable forms.
							</li>
							<li>
								Some examples of making something more searchable: stemming, lowercasing, and trimming.
							</li>
							<li>
								Example TokenFilters include: stop token, reverse token, snowball token,
								asciifolding.
							</li>
							<li>
								Example stop token:
								<pre><code data-trim>
['The', 'fox', 'is', 'brown', 'and', 'warm'] -> ['fox', 'brown', 'warm']
								</code></pre>
							</li>
							<li>
								There are 37 TokenFilters built into ElasticSearch natively, many provided by Lucene.
							</li>
						</ul>

						<aside class="notes">
							<li>
								TokenFilters get rid of all the noise in a search that we just don't need. The best example
								is a stop word. In the English language, a stop word is just a word that is used as
								a tool and really provides no semantic meaning to a sentence, such as and, or, or but.
								If we were to index this, it would become almost immediately useless due to how TF-IDF
								works, meaning it just takes up space. Well, instead of storing it... how about we
								strip it!? That's exactly what filters are for!
							</li>
							<li>
								Example TokenFilter:
								<ul>
									<li>
										Stop Token - removes stop words from a token stream. Examples include:
										and, is, and the. There are stop word dictionaries for a multitude of
										languages built into ES natively.
									</li>
									<li>
										Reverse Token - just reverses the token.
									</li>
									<li>
										Snowball Token - stems tokens using a Snowball generated stemming algorithm.
										Snowball is a small programming language geared around string properties
										and generating stemming algorithms.
									</li>
									<li>
										asciifolding - collapses unicode symbols above code point 127 into their
										ASCII counterparts. Example: Ã¼ turns into u.
									</li>
								</ul>
							</li>
						</aside>
					</section>

					<section>
						<h2>Stemming</h2>
						<ul>
							<li>
								Stemming is taking a word and reducing it to its "root stem".
							</li>
							<li>
								For example, consider the word: skiing. The root of skiing is really just ski, and almost all
								searches for ski or skiing should return both. Another example is "dogs", whose root is dog. A
								search for dogs or dog should almost always be stemmed down because rarely do people care how
								many adorable puppies are in their returned search results.
							</li>
						</ul>
					</section>

					<section>
						<h2>Stemming</h2>
						<ul>
							<li>
								Stemming rules differ per language, but are generally pretty easy.
							</li>
							<li>
								The algorithm is rather simple and basically works as a replacement table.
							</li>
						</ul>

						<table>
							<tr>
								<td>SSES		->		SS</td>
								<td>caresses		->		caress</td>
							</tr>
							<tr>
								<td>IES		->		I</td>
								<td>ponies		->		poni</td>
							</tr>
							<tr>
								<td>SS		->		SS</td>
								<td>caress		->		caress</td>
							</tr>
							<tr>
								<td>S		->	</td>
								<td>cats		->		cat</td>
							</tr>
						</table>

						<aside class="notes">
							Basically, you look at the end of words, and follow the rules.
							<br/>
							This is not all the rules for the English language, and you need to come up with
							rules for each language for this to work.
							<br/>
							This is called the Porter Stemming Algorithm and it is what Snowball implements and
							is quite good at.
						</aside>
					</section>

					<section>
						<img src="img/puppy.jpg" alt="A cute puppy."/>
						<aside class="notes">
							In fact, I searched for puppies on Google, and this single puppy is what showed up at the
							top, due to stemming.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Making Search Work for You</h2>

						<ul>
							<li>
								So far, ES has been quite powerful, but it's not perfect. There are some things ES
								doesn't handle <b>at all</b>, such as array ordering.
							</li>
							<li>
								There are always workarounds though!
							</li>
							<li>
								The solution to your problems in ES are almost always: STORE MORE DATA!
							</li>
						</ul>

						<aside class="notes">
							Internally, it would be an ENORMOUS pain in the ass for ES to store information on the
							ordering of each array entry, so it just <i>doesn't</i> (why? Because ES stores arrays as:
							{"user_alias": "bob"}, {"user_alias": "tim"}, {"user_alias": "joe"}, with just the array
							key, because this is easy to search over, storing information about the ordering of the array
							in the key would, by definition, make every key for every array entry different, no es bueno).
						</aside>
					</section>
					<section>
						<h2>Making Search Work for You</h2>
						<ul>
							<li>
								Let's fix our problem of searching over the first item in an array.
							</li>
							<li>
								We'll do this by simply indexing the first item in that array:
								<pre><code data-trim>
doc = create_doc(id)
doc.top_specialty = specialities[0]
index_doc(doc)
								</code></pre>
							</li>
						</ul>
					</section>

					<section>
						<h2>Making Search Work for You</h2>

							We can now search for everyone who's top specialty is <code>php</code> with:
							<pre><code data-trim>
hayden@beardtop ~> curl -XGET localhost:9200/croscon/employees/_search -d '
{
  "query": {
	"term": {
	  "top_specialty": "php"
	}
  }
}'
{
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{ "name": "Huck Finn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17", "top_specialty": "php" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "4",
      "_score" : 0.30685282,
      "_source":{ "name": "Pap Finn", "id": 4, "specialties": ["php", "python", "devops", "magic"], "date_of_birth": "1984-07-16", "top_specialty": "php" }
    } ]
  }
}

							</code></pre>
						<aside class="notes">
							But wait... why the score difference?
							<br/>
							While the IDF value was the same earlier, as I said, doc count is calculated PER SHARD
							and not PER INDEX. In this example, Huck lives on a shard that has two items in it, and
							Pap lives on a shard that has one item in it (just Pap), since IDF is the total number
							of documents divded by the frequency of a term, the score goes up as there are more documents!
							<br/>
							In reality, the distinction that document count is per shard and not per index has little
							meaning. You generally have so many items, and ES does a fine enough job of spreading them out,
							that this all evens out.
						</aside>
					</section>

					<section>
						<h2>Making Search Work for You</h2>
						<ul>
							<li>
								We've now made our search more robust and powerful!
							</li>
							<li>
								Need to see if a document has exact contents of an array? Alpha sort it, join them
								all with commas, and index that! Rebuild this at search time and... PROFIT!!!!
							</li>
							<li>
								If scoring doesn't work for you, make your <b>data</b> work for you.
							</li>
						</ul>

						<aside class="notes">
							No really, that's what an ES core contributor told me to do when I had this problem!
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Filters</h2>

						<ul>
							<li>
								Filters are basically queries that don't score anything.
							</li>
							<li>
								They run after queries, and filter the final resultset.
							</li>
							<li>
								They are excellent for removing items based on simple boolean tests, or exact
								matches.
							</li>
							<li>
								Since they don't score, they can be cached, unlike queries.
							</li>
							<li>
								Anytime you don't need to derive a score from a search condition, a filter
								should be used.
							</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>
									Filters are what make ES fast.
								</li>
								<li>
									In general, filters should always be preferred over queries, due to this caching
									mechanism.
								</li>
								<li>
									The only time you should use queries is when you want to score something.
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>Filter Caching</h2>
						<ul>
							<li>
								Many filters, such as term and prefix, are cached by default, others, such as geo and
								script, are not cached by default because the cost of caching them introduces additional
								processing overhead. Grouping filters, such as bool and and, themselves are not cached,
								but their internal filters frequently are.
							</li>
							<li>
								Either way, you can determine which filters are and are not cached for a given query,
								and even set the key they are cached under.
							</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>
									A good deal of your memory will be dedicated to your filter cache and managing
									your filter cache is the best way to improve ES performance.
								</li>
								<li>
									You can even cache the results of grouping filters, if you really want to!
								</li>
								<li>
									As stated, use filters over queries, whenever possible, due to this very caching
									reason.
								</li>
								<li>
									As you'll soon see, you can actually turn ANY query into a filter!
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>Example Filters</h2>
						<ul>
							<li>
								Some example filters include: and, bool, missing, and ids.
							</li>
							<li>
								An example:
							</li>
						</ul>
								<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon/employees/_search?pretty' -d '
{
  "query": {
    "filtered": {
      "query": { "match_all": {} },
      "filter": {
        "term": {
          "specialties": "php"
        }
      }
    }
  }
}'
{
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "4",
      "_score" : 1.0,
      "_source":{ "name": "Pap Finn", "id": 4, "specialties": ["php", "python", "devops", "magic"], "date_of_birth": "1984-07-16", "top_specialty": "php" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{ "name": "Tom Sawyer", "id": 1, "specialties": ["javascript", "php"], "date_of_birth": "1990-12-10" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{ "name": "Huck Finn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17", "top_specialty": "php" }
    } ]
  }
}
								</code></pre>
						<aside class="notes">
							<ul>
								<li>
									Example filters:
									<ul>
										<li>
											and - a filter that filters documents by joining multiple other filters
											together with a boolean `AND`.
										</li>
										<li>
											bool - similar in concept to the bool query, this filter filters out
											documents based on boolean combinations of other filters. must == and;
											should == or; must_not == negation.
										</li>
										<li>
											missing - a filter that filters documents based on the absence of a field.
											If the field is missing, the document matches. It's counterpart is exists.
										</li>
										<li>
											ids - a filter that lets you specify an id and a type, matching any document
											in that type with that id.
										</li>
									</ul>
								</li>
								<li>
									To use filters, you must use the filtered query, which will let you specify a
									query to run, and then a filter to filter those results through.
								</li>
								<li>
									Within the `filtered` query, you specify a single `query`, and a single `filter`.
									Much like with queries (as evinced earlier with the bool query), if you need to use
									multiple filters, then you use one of the grouping filters, such as and or bool,
									that let you decide which boolean opeartor joins each filter.
								</li>
								<li>
									Since filters are more performant and should be used over queries whenever
									possible, the `filtered` query is most likely the query you will use the most.
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>The Query Filter</h2>
						<ul>
							<li>
								A special filter that lets you turn any query into a filter, by running said query
								and merely discarding the scoring results.
							</li>
						</ul>

						<pre><code data-trim>
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "query": {
          "match": {
            "name": "Finn"
          }
        }
      }
    }
  }
}
						</code></pre>

						<aside class="notes">
							<ul>
								<li>
									Queries that are turned into filters lose the following features: scoring, and
									highlighting. Keep that in mind.
								</li>
								<li>
									Query filters are NOT cached by default, but this can be changed.
								</li>
								<li>
									In our example, we are using match as a filter, when match is not really a filter.
									@TODO: Flesh out. Maybe talk about match all.
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>A Complex Filter</h2>

						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon/employees/_search?pretty' -d '
{
 "query": {
   "filtered": {
     "filter": {
       "bool": {
         "must": {
           "query": {
             "bool": {
               "should": [
                 {
                   "match": {
                     "specialties": "css"
                   }
                 },
                  {
                      "match": {
                        "specialties": "php"
                      }
                  },
                  {
                     "match": {
                        "specialties": "javascript"
                      }
                  },
                  {
                    "match": {
                      "specialties": "python"
                    }
                  }
                ],
                "minimum_should_match": 2
              }
            }
          },
          "should": [
            {
              "exists": {
                "field": "date_of_birth"
              }
            },
            {
              "missing": {
                "field": "field_that_dne"
              }
            }
          ],
          "must_not": {
            "ids": {
              "type": "employees",
              "values": [1]
            }
          }
        }
      }
    }
  }
}'
{
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "4",
      "_score" : 1.0,
      "_source":{ "name": "Pap Finn", "id": 4, "specialties": ["php", "python", "devops", "magic"], "date_of_birth": "1984-07-16", "top_specialty": "php" }
    }, {
      "_index" : "croscon",
      "_type" : "employees",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{ "name": "HuckFinn", "id": 3, "specialties": ["php", "python", "devops"], "date_of_birth": "1990-10-17", "top_specialty": "php" }
    } ]
  }
}
						</code></pre>

						<aside class="notes">
							<ul>
								<li>
									Let's find everyone who has at least two specialties in: php, python, javascript,
									and css, who has the date_of_birth field, but not the `field_that_dne` field,
									and let's omit the employee with the id 1, if they happen to match.
								</li>
								<li>
									If you omit the query, like we did here, it is automatically assumed to be a `match_all`.
								</li>
								<li>
									Should's are great for scoring when used in queries, but their role changes when
									used in filters. You lose the ability to do `minimum_should_match` and they become
									one big OR condition. So, here we've made a `must` filter that is a `query` filter
									that then uses the bool query, which CAN use `minimum_should_match`.
								</li>
								<li>
									After running this, we only get Pap and Huck Finn back. Ben is omitted because he
									only has one matching specialty, and Tom Sawyer is gone because he has the ID of 1.
								</li>
								<li>
									While this is very complicated, it's result can actually be cached and this can be
									pretty performant.
								</li>
								<li>
									Notice that everything has a score of 1.0.
								</li>
							</ul>
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>A Complex Search</h2>

						<pre><code data-trim>
hayden@beardtop ~> curl -XGET localhost:9200/inventory/products,services/_search
{
    "sort": [{
        # list services over products
        "product_id": {
            "order": "desc",
            "missing": 9223372036854775806
        }
    },
    # sort by natural score and then name
    "_score",
     {
        "name.raw": {
            "order": "asc"
        }
    }],
    "query": {
        "filtered": {
            "query": {
                "function_score": {
                    "query": {
                        "function_score": {
                            "score_mode": "max",
                            "boost_mode": "replace",
                            "query": {
                                "bool": {
                                    # create an OR on products for a vendor and unfrozen services
                                    "should": [{
                                        "bool": {
                                            "must": [{
                                                "term": {
                                                    # only return products for this vendor
                                                    "vendor_id": {
                                                        "value": 160,
                                                        "boost": 1
                                                    }
                                                }
                                            }, {
                                                "term": {
                                                    "_type": {
                                                        # services don't have this field
                                                        "value": "products",
                                                        "boost": 1
                                                    }
                                                }
                                            }],
                                            "must_not": [{
                                                "term": {
                                                    # must be in stock
                                                    "out_of_stock": {
                                                        "value": true,
                                                        "boost": 1
                                                    }
                                                }
                                            }]
                                        }
                                    }, {
                                        # only return unfrozen services
                                        "bool": {
                                            "must": [{
                                                "term": {
                                                    "_type": {
                                                        "value": "services",
                                                        "boost": 1
                                                    }
                                                }
                                            }],
                                            # should not have frozen as true
                                            "must_not": [{
                                                "term": {
                                                    "frozen": {
                                                        "value": true,
                                                        "boost": 1
                                                    }
                                                }
                                            }]
                                        }
                                    }],
                                    "minimum_number_should_match": 1
                                }
                            },
                            # weight by category
                            "functions": [{
                                "weight": 13,
                                "filter": {
                                    "term": {
                                        "category_id": "18"
                                    }
                                }
                            },
                            {
                                "weight": 12,
                                "filter": {
                                    "term": {
                                        "category_id": "17"
                                    }
                                }
                            }]
                        }
                    },
                    # take the first score from a function, and
                    # then multiply the that with the score from the query
                    "score_mode": "first",
                    "boost_mode": "multiply",
                    "functions": [{
                        # if your stock is 0, we drop your score to 0
                        "weight": 0,
                        "filter": {
                            "term": {
                                "stock": 0
                            }
                        }
                    }, {
                        # heavily penalize services and products on sale
                        "weight": "0.000000000000000000000000000000000001",
                        "filter": {
                            "term": {
                                "on_sale": true
                            }
                        }
                    }]
                }
            },
            "filter": {
                "bool": {
                    "must": [{
                        "term": {
                            # must be present at this store
                            "store_id": 1
                        }
                    }],
                    "must_not": [{
                        "ids": {
                            # some services are sold as products as well, so we omit
                            # them this way
                            "type": "products",
                            "values": ["128", "531", "648", "1205", "1955", "1956", "2162", "3003", "4821", "7504", "7601", "7683", "8261", "8305", "8739", "8741", "8742", "8912", "8913", "8938", "9168", "9185", "9186", "9187", "9266", "9267", "9327", "9340", "9341", "9667", "11440", "15138", "15139", "15140", "16016", "16017", "16019", "16020", "16021", "16124", "24774", "24857", "27986", "27990", "29337", "31041", "31042", "31043", "31044", "31045", "32252", "36665", "36680", "42842", "42844", "42845", "43223", "47072", "47248", "50108", "50629", "50632", "50738", "50778", "52382", "52385", "52604", "54214", "55526", "55528", "55529", "55530", "55531", "55532", "55533", "55535", "55536", "55635", "57721", "57723", "57730", "2172", "49179", "57869", "57873", "688", "57912", "57914", "1172"]
                        }
                    }, {
                        "bool": {
                            "must": [{
                                # we don't show services that are at max capacity
                                "term": {
                                    "fully_booked": true
                                }
                            }, {
                                "type": {
                                    "value": "services"
                                }
                            }]
                        }
                    }]
                }
            }
        }
    },
    "post_filter": {
        "bool": {
            "must": [{
                "term": {
                    # when we're done aggregating, only return matches that
                    # are legal to sell in this country
                    "status": 1
                }
            }]
        }
    },
	# only pull the content field out
    "_source": ["content"],
	# 100 items please
    "size": "100",
    "from": 1,
    "aggs": {
        "starred-filter": {
            "filter": {
                "bool": {
                    "must": [{
                        "term": {
                            "status": 1
                        }
                    }, {
                        "term": {
                            "store_id": 1
                        }
                    }]
                }
            },
            # return an aggregation of all matches that have been starred, for
            # UI filtering purposes
            "aggs": {
                "starred": {
                    "terms": {
                        "field": "starred",
                        "size": 100
                    }
                }
            }
        }
    }
}
						</code></pre>
						<aside class="notes">
							<ul>
								<li>
									Let's search our inventory for products and services!
								</li>
								<li>
									We use a function score query to get the most customizable scoring
									possible. This example is decent at showing how queries can manipulate
									the original tf-idf score that Lucene returned.
								</li>
								<li>
									We also have some aggregations, for UI purposes.
								</li>
								<li>
									Orders are involved.
								</li>
								<li>
									This query could be better optimized in general, but it was adapted from
									a production query for example purposes.
								</li>
								<li>
									Also, this is the simplified form of the query! It's even more intense
								</li>
								<li>
									This is a great reason why you should use a Query Builder in elasticsearch. Writing
									this by hand would take ages and would be incredibly error prone.
								</li>
							</ul>
						</aside>
					</section>

					<section>
						<h2>Searching Over Multiple Indexes/Types</h2>
						<pre><code data-trim>
hayden@beardtop ~> curl -XGET 'localhost:9200/croscon,test/test,projects/_search?pretty'
{
  "hits" : {
    "total" : 6,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "AU-RxZ641Py57iUyrzuJ",
      "_score" : 1.0,
      "_source":{"foo": 1, "bar": 2}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "AU-Rxd1z1Py57iUyrzuK",
      "_score" : 1.0,
      "_source":{"foo": 1}
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "MC3 Rearch", "due_date": "2015-09-08", "id": 1, "client": { "name": "HFA", "id": 1 } }
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"foo": 1}
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"name": "MC4", "due_date": "2016-12-21", "id": 2, "client": { "name": "Croscon", "id": 2 } }
    }, {
      "_index" : "croscon",
      "_type" : "projects",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{"name": "MC5", "due_date": null, "id": 3, "client": null }
    } ]
  }
}
						</code></pre>
						<aside class="notes">
							<ul>
								<li>
									In that last example, we saw that you can search over two types at once.
									But you can take this further! You can search over multiple indexes, or
									even multiple indexes and types.
								</li>
							</ul>
						</aside>
					</section>
				</section>

				<section>
					<h2>Just the Beginning</h2>
					<ul>
						<li>
							This 38 slide presentation is really just the tip of the iceberg with elasticsearch.
						</li>
						<li>
							Other features not really covered include: aggregations, suggestors,
							highlighting, percolating, more like this, and search exists.
						</li>
						<li>
							There's also easy clustering, and we didn't even touch how ES really works.
						</li>
						<li>
							Oh, and there are plugins too.
						</li>
						<li>
							And you can write your own queries...
						</li>
						<li>
							I'll have to do another talk...
						</li>
					</ul>

					<aside class="notes">
						<ul>
							<li>
								Aggregations - counting, summing, grouping, etc. operations done on the
								data that we found. You then transform it into an aggregate statistic for
								displaying. Such as, number of vendors for products.
							</li>
							<li>
								Suggestors - suggestors take a query in, and then suggest terms that are
								indexed that are similar to the query.
							</li>
							<li>
								Highlighting - highlighting is when ES actually highlights the parts of
								a string that matched the query. By default, it wraps them in `em` tags.
							</li>
							<li>
								Percolating - a whole series of magic I don't even understand.
							</li>
							<li>
								More Like This - give it a document, returns similar documents.
							</li>
							<li>
								Search Exists - simply returns a true or false as to whether you would
								get any matches for a given search.
							</li>
						</ul>
					</aside>
				</section>

				<section>
					<h2>Under the Hood</h2>
					<ul>
						<li>
							@TODO: Explain Shards. Shards are really just individual instances of Lucene.
						</li>
						<li>
							@TODO: Explain clusters. Clusters are just groups of runnig elasticsearch clusters
							that can split out the work of indexing and searching for data. A master is appointed.
						</li>
						<li>@TODO: Show health check example</li>
					</ul>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
